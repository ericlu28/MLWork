{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.2640 - accuracy: 0.9243\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.1097 - accuracy: 0.9656\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0754 - accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1965641c7c8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_train, axis=1)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 60000 input samples and 10000 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3452f7a38776>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    831\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m   def predict(self,\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m     return self._model_iteration(\n\u001b[0;32m    455\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m         sample_weight=sample_weight, steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2532\u001b[0m       \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2533\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2534\u001b[1;33m         \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2535\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2536\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    675\u001b[0m                      \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m                      \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    678\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 60000 input samples and 10000 target samples."
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00393124 0.02332955 0.02620568 0.02625207 0.17420356 0.17566281\n",
      "  0.28629534 0.05664824 0.51877786 0.71632322 0.77892406 0.89301644\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.05780486 0.06524513 0.16128198 0.22713296\n",
      "  0.22277047 0.32790981 0.36833534 0.3689874  0.34978968 0.32678448\n",
      "  0.368094   0.3747499  0.79066747 0.67980478 0.61494005 0.45002403\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.12250613 0.45858525 0.45852825 0.43408872 0.37314701\n",
      "  0.33153488 0.32790981 0.36833534 0.3689874  0.34978968 0.32420121\n",
      "  0.15214552 0.17865984 0.25626376 0.1573102  0.12298801 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.04500225 0.4219755  0.45852825 0.43408872 0.37314701\n",
      "  0.33153488 0.32790981 0.28826244 0.26543758 0.34149427 0.31128482\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.1541463  0.28272888 0.18358693 0.37314701\n",
      "  0.33153488 0.26569767 0.01601458 0.         0.05945042 0.19891229\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.0253731  0.00171577 0.22713296\n",
      "  0.33153488 0.11664776 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.20500962\n",
      "  0.33153488 0.24625638 0.00291174 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01622378\n",
      "  0.24897876 0.32790981 0.10191096 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.04586451 0.31235677 0.32757096 0.23335172 0.14931733 0.00129164\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.10498298 0.34940902 0.3689874  0.34978968 0.15370495\n",
      "  0.04089933 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.06551419 0.27127137 0.34978968 0.32678448\n",
      "  0.245396   0.05882702 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02333517 0.12857881 0.32549285\n",
      "  0.41390126 0.40743158 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.32161793\n",
      "  0.41390126 0.54251585 0.20001074 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.06697006 0.18959827 0.25300993 0.32678448\n",
      "  0.41390126 0.45100715 0.00625034 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.05110617 0.19182076 0.33339444 0.3689874  0.34978968 0.32678448\n",
      "  0.40899334 0.39653769 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.04117838 0.16813739\n",
      "  0.28960162 0.32790981 0.36833534 0.3689874  0.34978968 0.25961929\n",
      "  0.12760592 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.04431706 0.11961607 0.36545809 0.37314701\n",
      "  0.33153488 0.32790981 0.36833534 0.28877275 0.111988   0.00258328\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.05298497 0.42752138 0.4219755  0.45852825 0.43408872 0.37314701\n",
      "  0.33153488 0.25273681 0.11646967 0.01312603 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.37491383 0.56222061\n",
      "  0.66525569 0.63253163 0.48748768 0.45852825 0.43408872 0.359873\n",
      "  0.17428513 0.01425695 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.92705966 0.82698729\n",
      "  0.74473314 0.63253163 0.4084877  0.24466922 0.22648107 0.02359823\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0], cmap = plt.cm.binary)\n",
    "plt.show()\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erica\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: numreader\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('numreader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('numreader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = new_model.predict([x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.18732126e-08 8.63440073e-06 2.12243026e-06 ... 1.51166125e-06\n",
      "  1.55426694e-08 1.58929799e-06]\n",
      " [9.99802649e-01 7.94334909e-08 1.87733836e-04 ... 2.34008141e-07\n",
      "  2.23777405e-08 2.46770060e-06]\n",
      " [1.09075145e-10 2.56467342e-06 3.81430378e-04 ... 1.50411521e-04\n",
      "  7.84507620e-07 7.18575666e-06]\n",
      " ...\n",
      " [1.87839383e-10 2.05234563e-09 1.42803590e-11 ... 1.04405226e-11\n",
      "  9.29859123e-09 9.52446783e-07]\n",
      " [7.24368510e-05 6.70909822e-06 1.07214255e-05 ... 5.39234406e-07\n",
      "  1.98706693e-05 2.09785227e-07]\n",
      " [5.79280953e-04 4.60859837e-07 1.06887150e-04 ... 3.27032240e-06\n",
      "  9.99126732e-01 1.42236837e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOfklEQVR4nO3dXYxc9XnH8d9vl/ULtnH8jmNMTIkDdSmYduu0uKpoURIgUiGREoULRCtU5yKoiZqLInoBl6jKi3LRRnKKFadKidIShNWiArWoKFFDWZCxTZyAeYmxsb28GGNe1l6vn17sgBbY+c8yc2bO4Of7kVYzc545cx6G/fnMzv+c83dECMDpb6DuBgD0BmEHkiDsQBKEHUiCsANJnNHLjc3y7Jijeb3cJJDKmN7UiTju6Wodhd32lZK+J2lQ0j9FxO2l58/RPH3aV3SySQAFj8T2prW2P8bbHpT0D5KukrRO0nW217X7egC6q5O/2TdI2hsRz0bECUk/kXRNNW0BqFonYV8l6YUpj/c3lr2H7U22R2yPjOt4B5sD0IlOwj7dlwAfOPY2IjZHxHBEDA9pdgebA9CJTsK+X9LqKY/PkfRiZ+0A6JZOwv6opLW2z7M9S9JXJG2rpi0AVWt76C0iTtq+SdJ9mhx62xIRT1bWGYBKdTTOHhH3Srq3ol4AdBGHywJJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BER7O44iPALpYHZs8urz/Q2f7g5B9c0Pa6R8+bU6wv/Y+9xfqe2z/RtPb5i3cV171o3v5i/YsLnirWr1+9sVivQ0dht/28pGOSJiSdjIjhKpoCUL0q9ux/GhEvV/A6ALqIv9mBJDoNe0i63/ZjtjdN9wTbm2yP2B4Z1/EONwegXZ1+jN8YES/aXi7pAdu/ioiHpj4hIjZL2ixJZ3lxdLg9AG3qaM8eES82bkcl3S1pQxVNAahe22G3Pc/2gnfuS/qspN1VNQagWp18jF8h6W5PjuOeIelfIuI/K+nqNDO4bFmx7jMGi/Xx81YU62PLmo+V+1RxVb1+bvlX4OS88vqtDIwXaifK67ba9r4b1xbrt278t6a1e1/+3eK6W0YvK9a/9cs/L9bP1/8W63VoO+wR8aykSyrsBUAXMfQGJEHYgSQIO5AEYQeSIOxAEpziWoGBiy4s1p/6y4919vonyqepzjravD70RosXr/OYxhbb/vhD5eYn5pR/fTc/98Wmtfn73i6uu+RY+dDuRbv7b2itFfbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+wV8IHDxfrg24uK9Ym5/XsBn7mj5d4GT5Trby1rvj8ZONniv/sXO8vbLq+tBS3qJS3ODP5IYs8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzl6BiSNHivU1//5msX7kgvI1k8/aVz63+tCny1Mbl8w5Uh5RXvivjxfrMV6+HvSiFcub1sYuObe4LqrFnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcvRdanJe9ZOeZxfqpt94q1heftaFp7djq8v/iRU+UjxGYaDGO3srE4dGmtaH7m9dQvZZ7dttbbI/a3j1l2WLbD9h+unFbvjoDgNrN5GP8DyVd+b5lN0vaHhFrJW1vPAbQx1qGPSIekvTq+xZfI2lr4/5WSddW3BeAirX7Bd2KiDgoSY3bpgdA295ke8T2yLjKx3gD6J6ufxsfEZsjYjgihoc0u9ubA9BEu2E/bHulJDVu+VoV6HPthn2bpBsa92+QdE817QDolpbj7LbvlHS5pKW290u6VdLtkn5q+0ZJ+yR9qZtNnu5ajaO3Mni8/evOH11Xnjt+/pNtvzT6TMuwR8R1TUpXVNwLgC7icFkgCcIOJEHYgSQIO5AEYQeS4BTX08Ds7U80rZ254NLium8tL098vPBT5xfrE089U6yjf7BnB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGc/DZSmTV74X08V1x378oXF+sHPrCjW5//O0mL9zAOF03f/b1dxXVSLPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4+2lu4kh5SuYVD5bn9zjw+fI4+2try+fDv75mQdPa2QMXF9cd/NVvivWJ144W63gv9uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7Mm1uu77OcfeLNZf+bM1xfrby5vvT0Z/f35x3XnnlM+1P+t/nivWJw6XjyHIpuWe3fYW26O2d09ZdpvtA7Z3NH6u7m6bADo1k4/xP5R05TTLvxsR6xs/91bbFoCqtQx7RDwk6dUe9AKgizr5gu4m2zsbH/MXNXuS7U22R2yPjOt4B5sD0Il2w/59SedLWi/poKRvN3tiRGyOiOGIGB7S7DY3B6BTbYU9Ig5HxEREnJL0A0kbqm0LQNXaCrvtlVMefkHS7mbPBdAfWo6z275T0uWSltreL+lWSZfbXi8pJD0v6atd7BE1OnnwULG+6K7XyvULzmtaO7Sx6Vc9kqSXLy7vi1656JPF+rm3Mc4+VcuwR8R10yy+owu9AOgiDpcFkiDsQBKEHUiCsANJEHYgCU5xRUdOjY2Vn/DEnqYlX/ZHHW37xPlvl+ufG25am3XfSEfb/ihizw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDOjqIzVp5drI+tW1WuLxlqWosBt9h6FKs+NKdYn3X/L1q8fi7s2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZT3ODS5cU68cvWVOsH15XnsVnYm55+wPjhdqJ8ro+VR6Hn/V6eX1FeZw+G/bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+wfAYMfW1isj1/UfFrkI58sD4SPzyuPZbcaR+/E3FdOFetnP3y0WD9VuCY9Pqjlnt32atsP2t5j+0nbX28sX2z7AdtPN27Lk20DqNVMPsaflPTNiPhtSX8o6Wu210m6WdL2iFgraXvjMYA+1TLsEXEwIh5v3D8maY+kVZKukbS18bStkq7tVpMAOvehvqCzvUbSpZIekbQiIg5Kk/8gSFreZJ1Ntkdsj4zreGfdAmjbjMNue76kuyR9IyJanYLwrojYHBHDETE8pPJJFQC6Z0Zhtz2kyaD/OCJ+1lh82PbKRn2lpNHutAigCi2H3mxb0h2S9kTEd6aUtkm6QdLtjdt7utLhaWBwUXmgIlatKNZfuGpxef3B5rWhN4qrtrpac0tzR8svsPSRl5vWJvY8XVy3PDCHD2sm4+wbJV0vaZftHY1lt2gy5D+1faOkfZK+1J0WAVShZdgj4mFJzY68uKLadgB0C4fLAkkQdiAJwg4kQdiBJAg7kASnuM5Q6ZLMe//mU8V13WLAeGJueax64ES5Putoq6mPmzvzpXJzC//7SLF+6tfPFOsTJ09+6J7QHezZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJNOPsB26+rFg/sb584veFK5tfm+PssUPFdffvnfaKXe8aGGt/nFySXBjKXrrr7eK6gz/fVawzTn76YM8OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mkGWff/df/WKzf/9ZQsf7zN5ufs/7gofL57LNfKlzYXdKS3RPF+sB4+ZzzefftbFo7NTZWXLfDy8bjI4Q9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kMZP52VdL+pGkszU5ZfbmiPie7dsk/ZWklxpPvSUi7u1Wo5363MfXd+215+q5Yv3cFvVOMY85ZmImB9WclPTNiHjc9gJJj9l+oFH7bkR8q3vtAajKTOZnPyjpYOP+Mdt7JK3qdmMAqvWh/ma3vUbSpZIeaSy6yfZO21tsL2qyzibbI7ZHxnW8o2YBtG/GYbc9X9Jdkr4REa9L+r6k8yWt1+Se/9vTrRcRmyNiOCKGhzS7gpYBtGNGYbc9pMmg/zgifiZJEXE4IiYi4pSkH0ja0L02AXSqZdhtW9IdkvZExHemLF855WlfkLS7+vYAVGUm38ZvlHS9pF22dzSW3SLpOtvrNXmW5POSvtqVDgFUYibfxj8saboLm/ftmDqAD+IIOiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKO6N2kvbZfkvSbKYuWSnq5Zw18OP3aW7/2JdFbu6rs7RMRsWy6Qk/D/oGN2yMRMVxbAwX92lu/9iXRW7t61Rsf44EkCDuQRN1h31zz9kv6tbd+7Uuit3b1pLda/2YH0Dt179kB9AhhB5KoJey2r7T9a9t7bd9cRw/N2H7e9i7bO2yP1NzLFtujtndPWbbY9gO2n27cTjvHXk293Wb7QOO922H76pp6W237Qdt7bD9p++uN5bW+d4W+evK+9fxvdtuDkp6S9BlJ+yU9Kum6iPhlTxtpwvbzkoYjovYDMGz/iaQ3JP0oIi5qLPt7Sa9GxO2NfygXRcTf9klvt0l6o+5pvBuzFa2cOs24pGsl/YVqfO8KfX1ZPXjf6tizb5C0NyKejYgTkn4i6Zoa+uh7EfGQpFfft/gaSVsb97dq8pel55r01hci4mBEPN64f0zSO9OM1/reFfrqiTrCvkrSC1Me71d/zfceku63/ZjtTXU3M40VEXFQmvzlkbS85n7er+U03r30vmnG++a9a2f6807VEfbpppLqp/G/jRHxe5KukvS1xsdVzMyMpvHulWmmGe8L7U5/3qk6wr5f0uopj8+R9GINfUwrIl5s3I5Kulv9NxX14Xdm0G3cjtbcz7v6aRrv6aYZVx+8d3VOf15H2B+VtNb2ebZnSfqKpG019PEBtuc1vjiR7XmSPqv+m4p6m6QbGvdvkHRPjb28R79M491smnHV/N7VPv15RPT8R9LVmvxG/hlJf1dHD036+i1JTzR+nqy7N0l3avJj3bgmPxHdKGmJpO2Snm7cLu6j3v5Z0i5JOzUZrJU19fbHmvzTcKekHY2fq+t+7wp99eR943BZIAmOoAOSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJP4fjiJD0fZ/ZJQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
